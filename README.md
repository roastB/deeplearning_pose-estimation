# 🏥 Fall Detection using GRU  

이 프로젝트는 addinedu-ros-7th_DL-Project에서 제가 맡아 진행한 AI 모델로, **GRU(Gated Recurrent Unit)**을 활용한 낙상 감지 AI 시스템입니다.

Mediapipe를 이용해 사람의 pose point (x, y) 데이터를 JSON 파일로 추출하고, 이를 기반으로 GRU 모델을 학습하였습니다.
이후, 학습된 낙상 감지 AI 모델을 적용하여 실시간으로 낙상 여부를 예측하며, 비상 상황으로 인식 시 자동으로 로그를 저장하고 긴급연락을 취하는 위험 상황 인식 기능을 포함하고 있습니다.

---

## 📂 프로젝트 개요  

### **1. 데이터 구성**  
- **낙상 데이터(Y):** 17,040개  
- **정상 데이터(N):** 5,680개  
- **총 데이터 수:** 22,720개  
- **입력 데이터:** Mediapipe의 pose point (33개 관절, x/y 2개의 좌표)  

### **2. 데이터 전처리**  

#### 📊 데이터 구조  
| 값  | 의미 |  
|------|---------------------------------|  
| **2272** | 시퀀스 개수 |  
| **10**   | 한 시퀀스당 포함된 프레임 수 |  
| **33**   | Mediapipe에서 추출한 관절 포인트 개수 |  
| **2**    | 각 포인트의 (x, y) 좌표 |

1. JSON 데이터를 변환하여 **(X, Y 값)** 형태로 변환
2. **Reshape 과정:**  
   - `(2272, 10, 33, 2)` → `(1136, 10, 66)` (33개 관절 x 2 좌표)
     : json 파일에서 누락된 데이터가 존재해서 시퀀스를 1/2만 사용. 
   - 레이블(Label)은 `(1136, )`  

---

## 🏋️‍♂️ 모델 학습  

- **GRU 모델**을 사용하여 시퀀스 데이터 기반 낙상 감지 모델을 학습  
- 학습된 모델을 저장하여 **실시간 감지**에 활용  

---

## 🎥 실시간 감지 프로세스  

1. **카메라 입력:**  
   - 실시간 영상에서 Mediapipe를 활용하여 pose point 추출  
   - `(33,2)` 형태의 좌표 데이터 생성  

2. **입력 데이터 변환:**  
   - 10프레임씩 시퀀스를 구성하여 `(None, 10, 66)` 형태로 변환  

3. **GRU 모델 예측:**  
   - 학습된 모델을 사용하여 낙상 여부를 판단  

4. **Emergency Model 연동:**  
   - 예측 결과를 바탕으로 추가 분석 및 긴급 상황 감지  

5. **DB 기록:**  
   - 비상 상황 발생 시 `event_log`에 기록  
   - 저장 정보:  
     - `event_time`: 발생 시간  
     - `event_log`: 낙상 감지 여부  

---
## 🌟 시연 영상
![NahonLab_DLproject-VEED](https://github.com/roastB/deeplearning_pose-estimation/raw/main/NahonLab_DLproject-VEED.gif)


---
## 🛠 사용 기술  

- **프레임워크:** TensorFlow / Keras
- **Pose Estimation:** Mediapipe  
- **데이터 처리:** NumPy, Pandas  
- **DB 저장:** MySQL 


